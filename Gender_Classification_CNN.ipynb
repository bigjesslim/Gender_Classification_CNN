{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"final_code.ipynb","provenance":[{"file_id":"1o5rsk-Ua_F8RBVycDhUbAzMyL1XzS-IT","timestamp":1636609299009},{"file_id":"1nJfpHIaOmLnWPWVFq5bouQkJt_c9W3oX","timestamp":1635992839914}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"JNrHYL1TNWd9"},"source":["import matplotlib.pyplot as plt\n","from matplotlib import image\n","import numpy as np\n","import pandas as pd\n","import albumentations as A\n","import scipy.stats as stats\n","\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.utils import to_categorical\n","from skimage.util import random_noise\n","from sklearn.model_selection import train_test_split\n","\n","from keras.backend import set_session\n","from keras.backend import clear_session\n","from keras.backend import get_session\n","\n","\n","from PIL import Image\n","import glob\n","import cv2\n","import os\n","import shutil\n","import gc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JNtriUgORp2I"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JJt9bC_T83Em"},"source":["# Visualization functions"]},{"cell_type":"code","metadata":{"id":"59v42C3423B_"},"source":["# For plotting accuracy and loss\n","def plot_metrics_v_epoch(history):\n","    '''Plots the metrics given the output from model.fit()'''\n","    plt.figure(figsize=(12,4))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(history.history['accuracy'])\n","    plt.plot(history.history['val_accuracy'])\n","    plt.title('Model Accuracy')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Test'], loc='upper right')\n","\n","    plt.subplot(1, 2, 2)\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('Model loss')\n","    plt.ylabel('Cross Entropy Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Test'], loc='upper right')\n","    \n","    plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Z6CHKInk8WD"},"source":["def plot_param_metrics(acc_dict, loss_dict):\n","    plt.figure(figsize=(12,4))\n","    plt.subplot(1, 2, 1)\n","    for key in acc_dict.keys():\n","      plt.plot(acc_dict[key])\n","    plt.title('Model Accuracy')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(list(acc_dict.keys()), loc='lower right')\n","\n","    plt.subplot(1, 2, 2)\n","    for key in loss_dict.keys():\n","      plt.plot(loss_dict[key])\n","    plt.title('Model loss')\n","    plt.ylabel('Cross Entropy Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(list(loss_dict.keys()), loc='upper right')\n","    \n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eo7afbE_j9EN"},"source":["# Callbacks"]},{"cell_type":"code","metadata":{"id":"FyNxN-ZaprBZ"},"source":["import time\n","from datetime import datetime\n","\n","class TimeTakenCallback(tf.keras.callbacks.Callback):\n","    '''Custom keras callback to implement the timer. Saves the time taken to train 1 epoch in a global variable called training_times'''\n","    def on_epoch_begin(self, epoch, logs=None):\n","        self.start = time.time()\n","    def on_epoch_end(self, epoch, logs=None):\n","        global training_times\n","        training_times.append(time.time() - self.start)\n","\n","\n","time_taken_callback = TimeTakenCallback()\n","early_stop_callback = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=2, restore_best_weights=True)\n","callback_list = [time_taken_callback, early_stop_callback]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZrDV66RFKd7B"},"source":["# Code to change image directory into its desired format"]},{"cell_type":"code","metadata":{"id":"302xZ0FXMlTZ"},"source":["IMAGE_DIR = '/content/drive/MyDrive/full_data/'\n","OLD_DIR = '/content/drive/MyDrive/aligned/'\n","\n","# list of image folders within aligned directory \n","image_folders = list(os.listdir(OLD_DIR))[1:]\n","# gets masterlist of images \n","image_names = []\n","if '.DS_Store' in image_folders:\n","    image_folders.remove('.DS_Store')\n","for folder in image_folders:\n","    if '@' in folder:\n","        directory = OLD_DIR + str(folder) +\"/\"\n","        image_names = image_names + list(os.listdir(directory))[1:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gnnhwRxWKl4S"},"source":["# original image directory - labelled as 'aligned' folder from Adience website (https://talhassner.github.io/home/projects/Adience/Adience-data.html#agegender)\n","# original directory - one folder containing mutiple folders with images \n","# required directory - one folder containing all images\n","\n","def move_images(image_folders, image_names, limit):\n","    ''' moves images out of smaller folders of old directory into new directory '''\n","    for i in range(0, limit):\n","      for filename in image_names[i]:\n","        face_id = filename.split('.')[1]\n","        original_image = filename.split('.')[2] + '.jpg'\n","        \n","        target = list(data.loc[(data['face_id']==int(face_id)) & (data['original_image']==original_image)]['gender'])[0]\n","        source = '/content/drive/MyDrive/aligned/' + image_folders[i] + \"/\" + filename\n","        destination = IMAGE_DIR + target + '/' + filename\n","        if target != 'u':\n","          shutil.copy(source, destination)\n","    return None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"riJAWclWLXt1"},"source":["move_images(image_folders, image_names, len(image_names))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n2raHIuyz55E"},"source":["# Importing dataset"]},{"cell_type":"code","metadata":{"id":"sSgh9klaes5M"},"source":["data = pd.read_csv('cleaned_dataset.csv', index_col=0)\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PXNBFCHMJlxm"},"source":["# 1: Benchmark - Gilnet"]},{"cell_type":"code","metadata":{"id":"DDRoSKTDiKjl"},"source":["def GilnetModel():\n","    model = tf.keras.Sequential([\n","    layers.InputLayer((256,256, 3)),\n","    layers.Rescaling(1./255.),\n","    layers.CenterCrop(227, 227),\n","    # first conv layer\n","    layers.Conv2D(96, 7, padding='valid'),\n","    # modified relu to leakyrelu\n","    layers.LeakyReLU(),\n","    layers.MaxPooling2D(pool_size=(3, 3), strides=2),\n","    layers.Lambda(tf.nn.local_response_normalization),  \n","    # second conv layer\n","    layers.Conv2D(256, 5, padding='valid'),\n","    layers.LeakyReLU(),\n","    layers.MaxPooling2D(pool_size=(3, 3), strides=2),\n","    layers.Lambda(tf.nn.local_response_normalization),\n","    # third conv layer\n","    layers.Conv2D(384, 3, padding='valid'),\n","    layers.LeakyReLU(),\n","    layers.MaxPooling2D(pool_size=(3, 3), strides=2),\n","    layers.Lambda(tf.nn.local_response_normalization),\n","    layers.Flatten(),\n","    # first fully connected layer \n","    layers.Dense(512),\n","    layers.LeakyReLU(),\n","    layers.Dropout(0.5),\n","    # second fully connected layer \n","    layers.Dense(512),\n","    layers.LeakyReLU(),\n","    layers.Dropout(0.5),\n","    # output layer\n","    layers.Dense(2, activation='softmax')\n","    ])\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cvw6VSoFkRQZ"},"source":["## **Full run**"]},{"cell_type":"code","metadata":{"id":"g-fpJsp9cb-y"},"source":["# setting parameters \n","bs = 32\n","epochs = 10 \n","fold = 0\n","lr = 0.001\n","training_times = []\n","\n","idg = ImageDataGenerator()\n","train_data = image_data.loc[image_data['fold']!=fold].reset_index(drop=True).drop('fold', axis=1)\n","test_data = image_data.loc[image_data['fold']==fold].reset_index(drop=True).drop('fold', axis=1)\n","train_data_generator = idg.flow_from_dataframe(train_data, directory = IMAGE_DIR, \n","                                                x_col = \"image_file\", y_col = \"gender\",\n","                                                class_mode = \"categorical\", shuffle = False, \n","                                                batch_size=bs, seed=42, target_size=(256, 256))\n","test_data_generator = idg.flow_from_dataframe(test_data, directory = IMAGE_DIR, \n","                                                x_col = \"image_file\", y_col = \"gender\",\n","                                                class_mode = \"categorical\", shuffle = False, \n","                                                batch_size=bs, seed=42, target_size=(256, 256))\n","\n","model = GilnetModel()\n","model.compile(optimizer=tf.keras.optimizers.Adam(lr),loss='categorical_crossentropy', metrics=['accuracy'])\n","history = model.fit(train_data_generator, steps_per_epoch=len(train_data)//bs, \n","                    validation_data=test_data_generator, validation_steps=len(test_data)//bs, \n","                    epochs=epochs, verbose=1, callbacks = time_taken_callback)\n","\n","keras.backend.clear_session()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1nSM4liUw0KD"},"source":["plot_metrics_v_epoch(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kw7BQgbB2btQ"},"source":["# 2: Transfer Learning Model "]},{"cell_type":"markdown","metadata":{"id":"XNc5T-EtQ402"},"source":["## **NASNetMobile**"]},{"cell_type":"code","metadata":{"id":"9i2p5SsW2l47"},"source":["def NASNetMobile_tfl():\n","  global NASNetMobile\n","  NASNetMobile = tf.keras.applications.NASNetMobile(\n","      input_shape=(224, 224, 3),\n","      include_top=False,\n","      weights=\"imagenet\",\n","      input_tensor=None,\n","      pooling=None\n","  )\n","  \n","  inputs = tf.keras.Input(shape=(224, 224, 3))\n","  x = NASNetMobile(inputs)\n","  # Convert features of shape `base_model.output_shape[1:]` to vectors\n","  x = layers.GlobalAveragePooling2D()(x)\n","  x = layers.Dense(128)(x)\n","  x = layers.LeakyReLU()(x)\n","  x = layers.Dense(128)(x)\n","  x = layers.LeakyReLU()(x)\n","  # A Dense classifier\n","  outputs = layers.Dense(2, activation='softmax')(x)\n","\n","  model = tf.keras.Model(inputs, outputs)\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JOEtBfm40w9V"},"source":["## **Parameter tuning**"]},{"cell_type":"code","metadata":{"id":"wevzTuaIzYl2"},"source":["# learning rate tuning on 5000 samples\n","lr_values = [0.01, 0.001, 0.0001, 0.00001]\n","lr_accuracies = {}\n","lr_losses = {}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gGhAkAKpydos"},"source":["# setting parameters \n","bs = 32\n","epochs = 10 \n","fold = 0\n","training_times = []\n","\n","idg = ImageDataGenerator(rescale=1./255)\n","train_data = image_data.loc[image_data['fold']!=fold].sample(n=4000, random_state=42).reset_index(drop=True).drop('fold', axis=1)\n","test_data = image_data.loc[image_data['fold']==fold].sample(n=1000, random_state=42).reset_index(drop=True).drop('fold', axis=1)\n","train_data_generator = idg.flow_from_dataframe(train_data, directory = IMAGE_DIR, \n","                                                x_col = \"image_file\", y_col = \"gender\",\n","                                                class_mode = \"categorical\", shuffle = False, \n","                                                batch_size=bs, seed=42, target_size=(256, 256))\n","test_data_generator = idg.flow_from_dataframe(test_data, directory = IMAGE_DIR, \n","                                                x_col = \"image_file\", y_col = \"gender\",\n","                                                class_mode = \"categorical\", shuffle = False, \n","                                                batch_size=bs, seed=42, target_size=(256, 256))\n","\n","for lr in lr_values:\n","  model = NASNetMobile_tfl()\n","  model.compile(optimizer=tf.keras.optimizers.Adam(lr),loss='categorical_crossentropy', metrics=['accuracy'])\n","  history = model.fit(train_data_generator, steps_per_epoch=len(train_data)//bs, \n","                      validation_data=test_data_generator, validation_steps=len(test_data)//bs, \n","                      epochs=epochs, verbose=1)\n","  \n","  lr_accuracies[lr] = history.history['val_accuracy']\n","  lr_losses[lr] = history.history['val_loss']\n","  keras.backend.clear_session()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V3LmCNJH25n3"},"source":["plot_param_metrics(lr_accuracies, lr_losses)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4spPeitmzpEv"},"source":["# batch size tuning on 5000 samples\n","bs_values = [8, 16, 32, 64]\n","bs_accuracies = {}\n","bs_losses = {}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C4ZsCLd9zrvT"},"source":["# setting parameters \n","lr = 0.00001\n","epochs = 5\n","fold = 0\n","training_times = []\n","\n","idg = ImageDataGenerator(rescale=1./255)\n","train_data = image_data.loc[image_data['fold']!=fold].sample(n=4000, random_state=42).reset_index(drop=True).drop('fold', axis=1)\n","test_data = image_data.loc[image_data['fold']==fold].sample(n=1000, random_state=42).reset_index(drop=True).drop('fold', axis=1)\n","\n","for bs in bs_values:\n","  train_data_generator = idg.flow_from_dataframe(train_data, directory = IMAGE_DIR, \n","                                                x_col = \"image_file\", y_col = \"gender\",\n","                                                class_mode = \"categorical\", shuffle = False, \n","                                                batch_size=bs, seed=42, target_size=(256, 256))\n","  test_data_generator = idg.flow_from_dataframe(test_data, directory = IMAGE_DIR, \n","                                                x_col = \"image_file\", y_col = \"gender\",\n","                                                class_mode = \"categorical\", shuffle = False, \n","                                                batch_size=bs, seed=42, target_size=(256, 256))\n","  model = NASNetMobile_tfl()\n","  model.compile(optimizer=tf.keras.optimizers.Adam(lr),loss='categorical_crossentropy', metrics=['accuracy'])\n","  history = model.fit(train_data_generator, steps_per_epoch=len(train_data)//bs, \n","                      validation_data=test_data_generator, validation_steps=len(test_data)//bs, \n","                      epochs=epochs, verbose=1)\n","  \n","  bs_accuracies[bs] = history.history['val_accuracy']\n","  bs_losses[bs] = history.history['val_loss']\n","  keras.backend.clear_session()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DDvvBu6QztuJ"},"source":["plot_param_metrics(bs_accuracies, bs_losses)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mIhzzndjQpdF"},"source":["## **Full run**"]},{"cell_type":"code","metadata":{"id":"HEPu9XutQnjZ"},"source":["# setting parameters \n","bs = 32\n","epochs = 10 \n","fold = 0\n","lr = 1e-5\n","training_times = []\n","\n","idg = ImageDataGenerator(rescale=1./255)\n","train_data = image_data.loc[image_data['fold']!=fold].reset_index(drop=True).drop('fold', axis=1)\n","test_data = image_data.loc[image_data['fold']==fold].reset_index(drop=True).drop('fold', axis=1)\n","train_data_generator = idg.flow_from_dataframe(train_data, directory = IMAGE_DIR, \n","                                                x_col = \"image_file\", y_col = \"gender\",\n","                                                class_mode = \"categorical\", shuffle = False, \n","                                                batch_size=bs, seed=42, target_size=(256, 256))\n","test_data_generator = idg.flow_from_dataframe(test_data, directory = IMAGE_DIR, \n","                                                x_col = \"image_file\", y_col = \"gender\",\n","                                                class_mode = \"categorical\", shuffle = False, \n","                                                batch_size=bs, seed=42, target_size=(256, 256))\n","\n","model = NASNetMobile_tfl()\n","model.compile(optimizer=tf.keras.optimizers.Adam(lr),loss='categorical_crossentropy', metrics=['accuracy'])\n","history = model.fit(train_data_generator, steps_per_epoch=len(train_data)//bs, \n","                    validation_data=test_data_generator, validation_steps=len(test_data)//bs, \n","                    epochs=epochs, verbose=1, callbacks = time_taken_callback)\n","\n","keras.backend.clear_session()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2orfiFGZBl4"},"source":["plot_metrics_v_epoch(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yYdPQ2h5ZC11"},"source":["## **DenseNet**"]},{"cell_type":"code","metadata":{"id":"wXx_WMN0ZBvN"},"source":["def DenseNet169_tfl():\n","  global DenseNet169\n","  DenseNet169 = tf.keras.applications.DenseNet169(\n","      input_shape=(224, 224, 3),\n","      include_top=False,\n","      weights=\"imagenet\",\n","      input_tensor=None,\n","      pooling=None\n","  )\n","  inputs = tf.keras.Input(shape=(224, 224, 3))\n","  x = DenseNet169(inputs)\n","  x = layers.GlobalAveragePooling2D()(x)\n","  x = layers.Dense(128)(x)\n","  x = layers.LeakyReLU()(x)\n","  x = layers.Dense(128)(x)\n","  x = layers.LeakyReLU()(x)\n","  outputs = layers.Dense(2, activation='softmax')(x)\n","\n","  model = tf.keras.Model(inputs, outputs)\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yj0gQn9ba1K4"},"source":["# setting parameters \n","bs = 32\n","epochs = 10 \n","fold = 0\n","lr = 1e-5\n","training_times = []\n","\n","idg = ImageDataGenerator(rescale=1./255)\n","train_data = image_data.loc[image_data['fold']!=fold].reset_index(drop=True).drop('fold', axis=1)\n","test_data = image_data.loc[image_data['fold']==fold].reset_index(drop=True).drop('fold', axis=1)\n","train_data_generator = idg.flow_from_dataframe(train_data, directory = IMAGE_DIR, \n","                                                x_col = \"image_file\", y_col = \"gender\",\n","                                                class_mode = \"categorical\", shuffle = False, \n","                                                batch_size=bs, seed=42, target_size=(256, 256))\n","test_data_generator = idg.flow_from_dataframe(test_data, directory = IMAGE_DIR, \n","                                                x_col = \"image_file\", y_col = \"gender\",\n","                                                class_mode = \"categorical\", shuffle = False, \n","                                                batch_size=bs, seed=42, target_size=(256, 256))\n","\n","model = DenseNet169_tfl()\n","model.compile(optimizer=tf.keras.optimizers.Adam(lr),loss='categorical_crossentropy', metrics=['accuracy'])\n","history = model.fit(train_data_generator, steps_per_epoch=len(train_data)//bs, \n","                    validation_data=test_data_generator, validation_steps=len(test_data)//bs, \n","                    epochs=epochs, verbose=1, callbacks = time_taken_callback)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XfOPZ28rbCZj"},"source":["plot_metrics_v_epoch(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kQUYlEfy02D3"},"source":["## **Finetuning**"]},{"cell_type":"code","metadata":{"id":"rFNtlstNkL7i"},"source":["bs = 32\n","epochs = 10\n","fold = 0\n","lr = 1e-5\n","idg = ImageDataGenerator(rescale=1./255)\n","train_data = image_data.loc[image_data['fold']!=fold].reset_index(drop=True).drop('fold', axis=1)\n","test_data = image_data.loc[image_data['fold']==fold].reset_index(drop=True).drop('fold', axis=1)\n","train_data_generator = idg.flow_from_dataframe(train_data, directory = IMAGE_DIR, \n","                                                x_col = \"image_file\", y_col = \"gender\",\n","                                                class_mode = \"categorical\", shuffle = False, \n","                                                batch_size=bs, seed=42, target_size=(256, 256))\n","test_data_generator = idg.flow_from_dataframe(test_data, directory = IMAGE_DIR, \n","                                                x_col = \"image_file\", y_col = \"gender\",\n","                                                class_mode = \"categorical\", shuffle = False, \n","                                                batch_size=bs, seed=42, target_size=(256, 256))\n","\n","# train only fully-connected layers first, with earlystopping\n","training_times = []\n","model = DenseNet169_tfl()\n","DenseNet169.trainable = False\n","model.compile(optimizer=tf.keras.optimizers.Adam(lr),loss='categorical_crossentropy', metrics=['accuracy'])\n","history_1 = model.fit(train_data_generator, steps_per_epoch=len(train_data)//bs, \n","                    validation_data=test_data_generator, validation_steps=len(test_data)//bs, \n","                    epochs=epochs, verbose=1, callbacks = callback_list)\n","\n","training_times_1 = training_times\n","\n","# then finetune entire model \n","DenseNet169.trainable = True # unfreeze layers\n","lr = 5e-6 # set smaller learning rate\n","epochs = 5\n","training_times = []\n","model.compile(optimizer=tf.keras.optimizers.Adam(lr),loss='categorical_crossentropy', metrics=['accuracy'])\n","history_2 = model.fit(train_data_generator, steps_per_epoch=len(train_data)//bs, \n","                    validation_data=test_data_generator, validation_steps=len(test_data)//bs, \n","                    epochs=epochs, verbose=1, callbacks = time_taken_callback)\n","training_times_2 = training_times\n","keras.backend.clear_session()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TLKtBcbcnnyL"},"source":["plot_metrics_v_epoch(history_1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cbk5uR7FySUD"},"source":["plot_metrics_v_epoch(history_2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"blkvqBWy0fLO"},"source":["# 3: Data Augmentation"]},{"cell_type":"markdown","metadata":{"id":"tAp2uIdZeqyl"},"source":["## Geometric augmentations"]},{"cell_type":"code","metadata":{"id":"lzBnuYU3ivlk"},"source":["def aug_function(x):\n","    new_x = x\n","    # augmentation layers added here\n","    new_x = layers.RandomFlip(mode=\"horizontal\", seed=42)(new_x)\n","    new_x = layers.RandomRotation(0.3, seed=42)(new_x)\n","    new_x = layers.RandomTranslation(0.2,0.2, seed=42)(new_x)\n","    return new_x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"REK2bxHoCSqL"},"source":["bs = 32\n","epochs = 10\n","fold = 0\n","lr = 1e-5\n","idg = ImageDataGenerator(rescale=1./255, preprocessing_function=aug_function)\n","train_data = image_data.loc[image_data['fold']!=fold].reset_index(drop=True).drop('fold', axis=1)\n","test_data = image_data.loc[image_data['fold']==fold].reset_index(drop=True).drop('fold', axis=1)\n","train_data_generator = idg.flow_from_dataframe(train_data, directory = IMAGE_DIR, \n","                                                x_col = \"image_file\", y_col = \"gender\",\n","                                                class_mode = \"categorical\", shuffle = False, \n","                                                batch_size=bs, seed=42, target_size=(256, 256))\n","test_data_generator = idg.flow_from_dataframe(test_data, directory = IMAGE_DIR, \n","                                                x_col = \"image_file\", y_col = \"gender\",\n","                                                class_mode = \"categorical\", shuffle = False, \n","                                                batch_size=bs, seed=42, target_size=(256, 256))\n","\n","training_times = []\n","\n","model = DenseNet169_tfl()\n","DenseNet169.trainable = False\n","model.compile(optimizer=tf.keras.optimizers.Adam(lr),loss='categorical_crossentropy', metrics=['accuracy'])\n","history = model.fit(train_data_generator, steps_per_epoch=len(train_data)//bs, \n","                    validation_data=test_data_generator, validation_steps=len(test_data)//bs, \n","                    epochs=epochs, verbose=1, callbacks = time_taken_callback)\n","\n","keras.backend.clear_session()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KXQV4ih6tVht"},"source":["plot_metrics_v_epoch(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FU9kxx5sfD9q"},"source":["## **Noise injection**"]},{"cell_type":"code","metadata":{"id":"mlJz-pErepUq"},"source":["def aug_function(x):\n","    new_x = x\n","\n","    # adding noise here\n","    new_x = random_noise(new_x,mode='speckle', mean=0, var=0.05, clip=True)\n","    new_x = random_noise(new_x, mode='s&p', salt_vs_pepper=0.5, clip=True)\n","    new_x = layers.GaussianNoise(0.05)(new_x)\n","\n","    return new_x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b_SpnIoqmDBK"},"source":["idg = ImageDataGenerator(rescale=1./255, preprocessing_function=aug_function)\n","train_data = image_data.loc[image_data['fold']!=fold].reset_index(drop=True).drop('fold', axis=1)\n","test_data = image_data.loc[image_data['fold']==fold].reset_index(drop=True).drop('fold', axis=1)\n","train_data_generator = idg.flow_from_dataframe(train_data, directory = IMAGE_DIR, \n","                                                x_col = \"image_file\", y_col = \"gender\",\n","                                                class_mode = \"categorical\", shuffle = False, \n","                                                batch_size=bs, seed=42, target_size=(256, 256))\n","test_data_generator = idg.flow_from_dataframe(test_data, directory = IMAGE_DIR, \n","                                                x_col = \"image_file\", y_col = \"gender\",\n","                                                class_mode = \"categorical\", shuffle = False, \n","                                                batch_size=bs, seed=42, target_size=(256, 256))\n","\n","bs = 32\n","epochs = 10\n","fold = 0\n","lr = 1e-5\n","training_times = []\n","DenseNet169.trainable = False\n","model = DenseNet169_tfl()\n","model.compile(optimizer=tf.keras.optimizers.Adam(lr),loss='categorical_crossentropy', metrics=['accuracy'])\n","history = model.fit(train_data_generator, steps_per_epoch=len(train_data)//bs, \n","                    validation_data=test_data_generator, validation_steps=len(test_data)//bs, \n","                    epochs=epochs, verbose=1, callbacks = time_taken_callback)\n","\n","keras.backend.clear_session()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DhV74Bn8NbfR"},"source":["plot_metrics_v_epoch(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mpFzxBamNTFq"},"source":["## **Brightness/Contrast/etc.**"]},{"cell_type":"code","metadata":{"id":"YowDIBPlNZ3g"},"source":["def aug_function(x):\n","    x = layers.Rescaling(scale=1./255)(x)\n","    new_x = x.numpy()\n","    transform = A.Compose([\n","        A.Blur(p=0.2),\n","        A.MotionBlur(p=0.2),\n","        A.HueSaturationValue(p=0.2),\n","        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.2)\n","    ])\n","    \n","    new_x = transform(image=new_x)['image']\n","    \n","    return tf.convert_to_tensor(new_x, dtype=tf.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E0mXzDcoR8Z4"},"source":["idg = ImageDataGenerator(preprocessing_function=aug_function)\n","train_data = image_data.loc[image_data['fold']!=fold].reset_index(drop=True).drop('fold', axis=1)\n","test_data = image_data.loc[image_data['fold']==fold].reset_index(drop=True).drop('fold', axis=1)\n","train_data_generator = idg.flow_from_dataframe(train_data, directory = IMAGE_DIR, \n","                                                x_col = \"image_file\", y_col = \"gender\",\n","                                                class_mode = \"categorical\", shuffle = False, \n","                                                batch_size=bs, seed=42, target_size=(256, 256))\n","test_data_generator = idg.flow_from_dataframe(test_data, directory = IMAGE_DIR, \n","                                                x_col = \"image_file\", y_col = \"gender\",\n","                                                class_mode = \"categorical\", shuffle = False, \n","                                                batch_size=bs, seed=42, target_size=(256, 256))\n","\n","bs = 32\n","epochs = 10\n","fold = 0\n","lr = 1e-5\n","training_times = []\n","DenseNet169.trainable = False\n","model = DenseNet169_tfl()\n","model.compile(optimizer=tf.keras.optimizers.Adam(lr),loss='categorical_crossentropy', metrics=['accuracy'])\n","history = model.fit(train_data_generator, steps_per_epoch=len(train_data)//bs, \n","                    validation_data=test_data_generator, validation_steps=len(test_data)//bs, \n","                    epochs=epochs, verbose=1, callbacks = time_taken_callback)\n","\n","keras.backend.clear_session()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WuVUuXQTwrK7"},"source":["plot_metrics_v_epoch(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZxgkvxqzptMm"},"source":["# 4: Test set oversampling"]},{"cell_type":"code","metadata":{"id":"Phf9lyx6vuMc"},"source":["def crop_generator(batches, crop_length, position):\n","    ''' generates the relevant crop of the test image based on position specified '''\n","    y_start = [16, 0, 31, 0, 31] \n","    y_end = [240, 224, 255, 224, 255]\n","    x_start = [16, 0, 0, 31, 31]\n","    x_end = [240, 224, 224, 255, 255]\n","    while True:\n","        batch_x, batch_y = next(batches)\n","        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n","        for i in range(batch_x.shape[0]):\n","            batch_crops[i] = random_crop(batch_x[i], x_start[position], x_end[position], y_start[position], y_end[position])\n","        yield (batch_crops, batch_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pgy9ujzDvtiL"},"source":["def random_crop(img, x_1, x_2, y_1, y_2):\n","    assert img.shape[2] == 3\n","    return img[y_1:(y_2), x_1:x_2, :]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sq-nmru3dtcL"},"source":["def calculate_accuracy(predicted, actual):\n","  ''' used calculate validation accuracy using prediction and actual labels '''\n","  actual = actual[0:len(predicted)-1]\n","  num_errors = 0\n","  for p, a in zip(predicted, actual):\n","    if p != a:\n","      num_errors += 1\n","  return (1-num_errors/len(predicted))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B_WIo6p5wUMC"},"source":["def get_val_accuracy(model):\n","  predictions = []\n","  fold = 0\n","  bs = 32\n","  global position \n","\n","  for position in range(0, 5):\n","    idg = ImageDataGenerator(rescale=1./255)\n","    test_data = image_data.loc[image_data['fold']==fold].reset_index(drop=True).drop('fold', axis=1)\n","    test_data_generator = idg.flow_from_dataframe(test_data, directory = IMAGE_DIR, \n","                                                    x_col = \"image_file\", y_col = \"gender\",\n","                                                    class_mode = \"categorical\", shuffle = False, \n","                                                    batch_size=bs, seed=42)\n","    test_crops = crop_generator(test_data_generator, 224, position)\n","    new_prediction = model.predict(test_crops, batch_size=bs, verbose=1, steps=len(test_data)//bs)\n","    gender_dict = {'m':1, 'f':0}\n","    labels = [*map(gender_dict.get, test_data['gender'])]\n","    current_predictions = np.argmax(new_prediction, axis=-1)\n","    predictions.append(new_prediction)\n","\n","  mean_predictions = np.mean(predictions, axis=0)\n","  final_predictions = np.argmax(mean_predictions, axis=-1)\n","  val_accuracy = calculate_accuracy(final_predictions, labels)\n","  return val_accuracy\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qjOLCngpZJAQ"},"source":["# setting parameters \n","bs = 32\n","epochs = 10\n","fold = 0\n","lr = 1e-5\n","training_times = []\n","\n","idg = ImageDataGenerator(rescale=1./255)\n","train_data = image_data.loc[image_data['fold']!=fold].reset_index(drop=True).drop('fold', axis=1)\n","train_data_generator = idg.flow_from_dataframe(train_data, directory = IMAGE_DIR, \n","                                                x_col = \"image_file\", y_col = \"gender\",\n","                                                class_mode = \"categorical\", shuffle = False, \n","                                                batch_size=bs, seed=42)\n","# compiling model\n","model = DenseNet169_tfl()\n","DenseNet169.trainable = False # freeze the conv layers\n","model.compile(optimizer=tf.keras.optimizers.Adam(lr),loss='categorical_crossentropy', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l54EN62TX50G"},"source":["# train without validation for 5 epochs (due to resource constraints in computing validation accuracy via oversampling)\n","history = model.fit(train_data_generator, steps_per_epoch=len(train_data)//bs, \n","                    epochs=5, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WixQfkxflUj0"},"source":["accuracy = []\n","val_accuracy = []\n","# with test set oversampling\n","for i in range (0, 5):\n","  history = model.fit(train_data_generator, steps_per_epoch=len(train_data)//bs, \n","                    epochs=1, verbose=1, callbacks = [time_taken_callback])\n","  accuracy.append(history.history['accuracy'])\n","  val_accuracy.append(get_val_accuracy(model))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PINdohNQI0rn"},"source":["plt.figure(figsize=(12,4))\n","plt.subplot(1, 2, 1)\n","plt.plot(accuracy)\n","plt.plot(val_accuracy)\n","plt.title('Model Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper right')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-KCqAQ7T-Q55"},"source":["# 5: Final model"]},{"cell_type":"code","metadata":{"id":"-1RrzazAB3lw"},"source":["def aug_function(x):\n","    x = layers.Rescaling(scale=1./255)(x)\n","    new_x = x.numpy()\n","    transform = A.Compose([\n","        A.Blur(p=0.2),\n","        A.MotionBlur(p=0.2),\n","        A.HueSaturationValue(p=0.2),\n","        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.2)\n","    ])\n","    \n","    new_x = transform(image=new_x)['image']\n","    \n","    return tf.convert_to_tensor(new_x, dtype=tf.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kNwQjiLq-Tqy"},"source":["# setting parameters \n","bs = 32\n","fold = 0\n","lr = 1e-5\n","training_times = []\n","\n","idg = ImageDataGenerator(preprocessing_function=aug_function)\n","train_data = image_data.loc[image_data['fold']!=fold].reset_index(drop=True).drop('fold', axis=1)\n","train_data_generator = idg.flow_from_dataframe(train_data, directory = IMAGE_DIR, \n","                                                x_col = \"image_file\", y_col = \"gender\",\n","                                                class_mode = \"categorical\", shuffle = False, \n","                                                batch_size=bs, seed=42)\n","\n","model = DenseNet169_tfl()\n","DenseNet169.trainable = False # freeze the conv layers\n","model.compile(optimizer=tf.keras.optimizers.Adam(lr),loss='categorical_crossentropy', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bzJ7JzIKAuVZ"},"source":["# before finetuning\n","accuracy = []\n","val_accuracy = []\n","# 10 epochs\n","for i in range (0, 10):\n","  history = model.fit(train_data_generator, steps_per_epoch=len(train_data)//bs, \n","                    epochs=1, verbose=1, callbacks = [time_taken_callback])\n","  accuracy.append(history.history['accuracy'])\n","  # get validation accuracy every epoch\n","  new_val_accuracy = get_val_accuracy(model)\n","  val_accuracy.append(new_val_accuracy)\n","  # earlystopping with patience = 1\n","  if (val_accuracy[-1] <  val_accuracy[-2]):\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DOasR-LaAx2Q"},"source":["# finetuning params and compilation\n","DenseNet169.trainable = True \n","lr = 5e-6\n","model.compile(optimizer=tf.keras.optimizers.Adam(lr),loss='categorical_crossentropy', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v76HhEXmBVy5"},"source":["ft_accuracy = []\n","ft_val_accuracy = []\n","# 5 epochs for finetuning\n","for i in range (0, 5):\n","  history = model.fit(train_data_generator, steps_per_epoch=len(train_data)//bs, \n","                    epochs=1, verbose=1, callbacks = [time_taken_callback])\n","  ft_accuracy.append(history.history['accuracy'])\n","  # every epoch get validation accuracy\n","  ft_val_accuracy.append(get_val_accuracy(model))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fgm-Ge-bogg6"},"source":["# transfer learning (before finetuning) plot\n","plt.plot(accuracy)\n","plt.plot(val_accuracy)\n","plt.title('Model Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper right')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M8LscTqgozD9"},"source":["# finetuning plot\n","plt.plot(ft_accuracy)\n","plt.plot(ft_val_accuracy)\n","plt.title('Model Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper right')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KEY6mf9CV-Ks"},"source":[""],"execution_count":null,"outputs":[]}]}